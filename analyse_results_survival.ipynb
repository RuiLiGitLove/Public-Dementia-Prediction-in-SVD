{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import seaborn as sns \n",
    "import pickle as pkl\n",
    "from Utils.data_preparation import get_feature_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Results with Multiple Feature Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot average results in heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['CoxPH', 'Reg_Cox', 'RSF', 'GBT']\n",
    "evaluation_strategy = 'all_pooled' \n",
    "\n",
    "metrics_to_report = ['Harrell_C', 'IBS'] \n",
    "feature_sets = ['Multi', 'Cog_Demo', 'Image_Demo', 'Demo'] \n",
    "feature_set_names = {\n",
    "    'Multi': 'All',\n",
    "    'Cog_Demo': 'Cog_Demo',\n",
    "    'Image_Demo': 'Image_Demo',\n",
    "    'Demo': 'Demo'\n",
    "}\n",
    "\n",
    "imputation_method = 'Complete_Case' #['Complete_Case', 'KNN_imputer_no_outcome', 'KNN_imputer_with_outcome', 'MF_imputer_no_outcome', 'MF_imputer_with_outcome']\n",
    "\n",
    "if evaluation_strategy == 'external_harmo_split':\n",
    "    dataset_names = {\n",
    "        'train_valid': 'Training',\n",
    "        'test': 'Internal Validation',\n",
    "        'harmo': 'HARMO',\n",
    "        'harmo_high_SVD': 'HARMO High SVD',\n",
    "        'harmo_low_SVD': 'HARMO Low SVD',\n",
    "    }\n",
    "elif evaluation_strategy == 'all_pooled':\n",
    "    dataset_names = {\n",
    "        'train_valid': 'Training',\n",
    "        'test': 'Testing'#'Internal Validation'\n",
    "    }\n",
    "\n",
    "full_results = []\n",
    "for model in models:\n",
    "    for feature_set in feature_sets:\n",
    "        this_dict_entry = {\n",
    "            'Model': model,\n",
    "            'Feature Set': feature_set_names[feature_set]\n",
    "        }\n",
    "\n",
    "        # Read in full validation results\n",
    "        input_variables_to_print, FS_name, var_description, cat_feature_indices = get_feature_set(feature_set)\n",
    "        folderpath = '/Users/lirui/Downloads/Cohort_Dementia_Prediction/Survival/Nested_CV_Results/{}/{}/{}/{}'.format(imputation_method, evaluation_strategy, model, FS_name)\n",
    "        results_for_this_model_FS = pkl.load( open(folderpath+'/{}_{}.pkl'.format(model, FS_name), 'rb') )['full_validation_results']\n",
    "\n",
    "        for dataset in list(dataset_names.keys()):\n",
    "            for metric in metrics_to_report:\n",
    "                for value in ['mean', 'std']:\n",
    "                    column_name = '{}_{}_{}'.format(dataset, metric, value)\n",
    "                    this_dict_entry[column_name] = results_for_this_model_FS[column_name].iloc[0]\n",
    "\n",
    "                column_name = '{}_{}_values'.format(dataset, metric)\n",
    "                this_dict_entry[column_name] = results_for_this_model_FS[column_name].iloc[0]\n",
    "\n",
    "                column_name = '{}_{}_mean_std'.format(dataset, metric)\n",
    "                this_dict_entry[column_name] = '{:.3f} ({:.3f})'.format(\n",
    "                    this_dict_entry['{}_{}_mean'.format(dataset, metric)],\n",
    "                    this_dict_entry['{}_{}_std'.format(dataset, metric)]\n",
    "                )\n",
    "\n",
    "        full_results.append(this_dict_entry)\n",
    "full_results_df = pd.DataFrame(full_results)\n",
    "full_results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "print_dataset={\n",
    "    'train': 'Training',\n",
    "    'test': 'Test'\n",
    "}\n",
    "\n",
    "dataset_to_report = 'test'\n",
    "col_param = 'Feature Set'\n",
    "column_order = [feature_set_names[fs] for fs in feature_sets]\n",
    "idx_param = 'Model'\n",
    "index_order = models\n",
    "\n",
    "fig, axes = plt.subplots(1, len(metrics_to_report), figsize=(5.5*len(metrics_to_report),5))\n",
    "for i, metric in enumerate(metrics_to_report):\n",
    "    \n",
    "\n",
    "    results_table = pd.pivot_table(full_results_df, values=dataset_to_report+'_'+metric+'_mean', columns=col_param, index=idx_param)\n",
    "    results_table = results_table.reindex(index_order, axis=0)\n",
    "    results_table = results_table.reindex(column_order, axis=1)\n",
    "\n",
    "    if metric == 'IBS':\n",
    "        axes[i] = sns.heatmap(results_table, ax=axes[i], annot=True, fmt='.3f', cmap='Greens') #sns.cm.rocket_r)\n",
    "    else:\n",
    "        axes[i] = sns.heatmap(results_table, ax=axes[i], annot=True, fmt='.3f', cmap='Greens_r') #sns.cm.rocket) \n",
    "    \n",
    "    if metric == 'Harrell_C':\n",
    "        axes[i].set_title('{} C-Index '.format(print_dataset[dataset_to_report]), fontsize=14, fontweight='bold')\n",
    "    else:\n",
    "        axes[i].set_title('{} {} '.format(print_dataset[dataset_to_report],metric), fontsize=14, fontweight='bold')\n",
    "\n",
    "    \n",
    "    axes[i].set_xlabel(col_param, fontsize=14, fontweight='bold')\n",
    "    axes[i].set_ylabel(idx_param, fontsize=14, fontweight='bold')\n",
    "\n",
    "    tl = axes[i].get_yticklabels()\n",
    "    axes[i].set_yticklabels(tl, rotation=0)\n",
    "\n",
    "    tl = axes[i].get_xticklabels()\n",
    "    axes[i].set_xticklabels(tl, rotation=45)\n",
    "#plt.yticks(rotation=0) \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('/Users/lirui/Downloads/Cohort_Dementia_Prediction/Survival/Nested_CV_Results/Complete_Case/all_pooled/test_results.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dataset = 'test'\n",
    "selected_metric = 'IBS'\n",
    "\n",
    "col_param = 'Feature Set'\n",
    "column_order = feature_sets\n",
    "idx_param = 'Model'\n",
    "index_order = models\n",
    "\n",
    "results_table = pd.pivot_table(full_results_df, values=selected_dataset+'_'+selected_metric+'_mean_std', columns=col_param, index=idx_param, aggfunc=lambda x: ' '.join(str(v) for v in x))\n",
    "results_table = results_table.reindex(index_order, axis=0)\n",
    "results_table = results_table.reindex(column_order, axis=1)\n",
    "\n",
    "print(selected_dataset, selected_metric)\n",
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for significant difference between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "model_1 = 'CoxPH'\n",
    "model_2 = 'GBT'\n",
    "\n",
    "print(model_1, model_2)\n",
    "for dataset in ['test']:\n",
    "    for metric in metrics_to_report:\n",
    "        for feature_set in feature_sets:\n",
    "            \n",
    "            row_for_model_1 = full_results_df.loc[(full_results_df['Model']==model_1) & (full_results_df['Feature Set']==feature_set)]\n",
    "            \n",
    "            row_for_model_2 = full_results_df.loc[(full_results_df['Model']==model_2) & (full_results_df['Feature Set']==feature_set)]\n",
    "\n",
    "            column_name = '{}_{}_values'.format(dataset, metric)\n",
    "            values_1 = row_for_model_1[column_name].iloc[0]\n",
    "            values_2 = row_for_model_2[column_name].iloc[0]\n",
    "\n",
    "            assert len(values_1) == len(values_2) == 5\n",
    "            \n",
    "            statistic, pval = ttest_rel(values_1, values_2)\n",
    "            if pval <0.05: # statistic<0 indicates values_1 less than values_2\n",
    "                if statistic<0:\n",
    "                    print('{}\\t{}\\t{}\\t pval:{}\\t {} larger'.format(dataset, metric, feature_set, pval, model_2))\n",
    "                else:\n",
    "                    print('{}\\t{}\\t{}\\t pval:{}\\t {} larger'.format(dataset, metric, feature_set, pval, model_1))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract individual results entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns_pallete = {\n",
    "#     'Logistic': sns.color_palette()[0],\n",
    "#     'Reg_Logistic': sns.color_palette()[2],\n",
    "#     'GMLVQ': sns.color_palette()[1]}\n",
    "\n",
    "models = ['CoxPH', 'Reg_Cox', 'RSF', 'GBT']\n",
    "evaluation_strategy = 'all_pooled' # Choose among: external_harmo, external_harmo_split, RUN_DMC_SCANS_only, all_pooled\n",
    "\n",
    "metrics_to_report = ['Harrell_C', 'IBS'] #['Harrell_C', 'Uno_C', 'IBS']\n",
    "feature_sets = ['Multi', 'Cog_Demo', 'Image_Demo', 'Demo'] # 'Image_Demo_no_PSMD'\n",
    "imputation_methods = ['Complete_Case'] #['Complete_Case', 'KNN_imputer_no_outcome', 'KNN_imputer_with_outcome', 'MF_imputer_no_outcome', 'MF_imputer_with_outcome']\n",
    "\n",
    "if evaluation_strategy == 'external_harmo_split':\n",
    "    dataset_names = {\n",
    "        'train_valid': 'Training',\n",
    "        'test': 'Internal Validation',\n",
    "        'harmo': 'HARMO',\n",
    "        'harmo_high_SVD': 'HARMO High SVD',\n",
    "        'harmo_low_SVD': 'HARMO Low SVD',\n",
    "    }\n",
    "elif evaluation_strategy == 'all_pooled':\n",
    "    dataset_names = {\n",
    "        'train_valid': 'Training',\n",
    "        'test': 'Testing'#'Internal Validation'\n",
    "    }\n",
    "\n",
    "model_FS_dataset_metric_imputation_df_lst = []\n",
    "for imputation_method in imputation_methods:\n",
    "    model_FS_dataset_metric_df_lst = []\n",
    "    for model in models:\n",
    "        FS_dataset_metric_df_lst = []\n",
    "        for FS_idx, this_feature_set in enumerate(feature_sets):\n",
    "            # Read in full validation results\n",
    "            input_variables_to_print, FS_name, var_description, cat_feature_indices = get_feature_set(this_feature_set)\n",
    "            folderpath = '/Users/lirui/Downloads/Cohort_Dementia_Prediction/Survival/Nested_CV_Results/{}/{}/{}/{}'.format(imputation_method, evaluation_strategy, model, FS_name)\n",
    "            this_results_dict = pkl.load( open(folderpath+'/{}_{}.pkl'.format(model, FS_name), 'rb') )\n",
    "            this_validation_results = this_results_dict['full_validation_results']\n",
    "\n",
    "            dataset_metric_df_lst = []\n",
    "            for dataset in list(dataset_names.keys()):\n",
    "                dataset_metric_df = {}\n",
    "                for metric in metrics_to_report:\n",
    "                    # if metric == 'Harrell_C':\n",
    "                    #     dataset_metric_df['C-Index'] = this_validation_results[dataset+'_'+metric+'_values'].iloc[0]\n",
    "                    # else:\n",
    "                    dataset_metric_df[metric] = this_validation_results[dataset+'_'+metric+'_values'].iloc[0]\n",
    "\n",
    "                dataset_metric_df = pd.DataFrame.from_dict(dataset_metric_df)\n",
    "                dataset_metric_df['Dataset'] = dataset_names[dataset]\n",
    "                dataset_metric_df_lst.append(dataset_metric_df)\n",
    "            \n",
    "            FS_dataset_metric_df = pd.concat(dataset_metric_df_lst)\n",
    "            FS_dataset_metric_df['Feature Set'] = this_feature_set\n",
    "            FS_dataset_metric_df_lst.append(FS_dataset_metric_df)\n",
    "        model_FS_dataset_metric_df = pd.concat(FS_dataset_metric_df_lst)\n",
    "        model_FS_dataset_metric_df['Model'] = model\n",
    "        model_FS_dataset_metric_df_lst.append(model_FS_dataset_metric_df)\n",
    "    model_FS_dataset_metric_imputation_df = pd.concat(model_FS_dataset_metric_df_lst)\n",
    "    model_FS_dataset_metric_imputation_df['Imputation'] = imputation_method\n",
    "    model_FS_dataset_metric_imputation_df_lst.append(model_FS_dataset_metric_imputation_df)\n",
    "\n",
    "complete_df = pd.concat(model_FS_dataset_metric_imputation_df_lst)\n",
    "print(complete_df.shape)\n",
    "complete_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1.2)\n",
    "metrics_to_plot = metrics_to_report\n",
    "models_to_print = models\n",
    "datasets = list(dataset_names.keys())\n",
    "fig, axes = plt.subplots(len(metrics_to_plot), len(datasets), figsize=(5.8*len(datasets),4.7*len(metrics_to_plot)), sharex=False)\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    for j, dataset in enumerate(datasets):\n",
    "        results_for_this_dataset = complete_df.loc[complete_df['Dataset']==dataset_names[dataset]]\n",
    "        sns.boxplot(x=\"Feature Set\", y=metric, hue=\"Model\", hue_order=models_to_print, data=results_for_this_dataset, ax=axes[i,j], showfliers=False, width=0.5)# fliersize=2\n",
    "        axes[i,j].set_title(dataset_names[dataset], fontweight='bold')\n",
    "        # sns.pointplot(x=\"Feature Set\", y=metric, hue=\"Model\", hue_order=models_to_print, data=results_for_this_dataset, ax=axes[i,j], showfliers=False, join=True, errwidth=1)\n",
    "        if i != len(datasets)-1:\n",
    "            axes[i,j].set_xlabel('')\n",
    "        else:\n",
    "            axes[i,j].set_xlabel('Feature Set', fontweight='bold')\n",
    "        \n",
    "        if metric == 'Harrell_C':\n",
    "            axes[i,j].set_ylabel('C-Index', fontweight='bold')\n",
    "        else:\n",
    "            axes[i,j].set_ylabel(metric, fontweight='bold')\n",
    "\n",
    "        #if (i==1 and j==(len(datasets)-1))==False:\n",
    "        axes[i,j].get_legend().remove()\n",
    "        \n",
    "        if metric=='IBS':\n",
    "            axes[i,j].set_ylim([0.02,0.1])\n",
    "        else:\n",
    "            axes[i,j].set_ylim([0.6,1.0])\n",
    "        #axes[i,j].set_title(dataset_names[dataset])\n",
    "        plt.setp(axes[i,j].get_xticklabels(), rotation=36, ha=\"right\",rotation_mode=\"anchor\")\n",
    "axes[0,len(datasets)-1].legend(bbox_to_anchor=(1.05, 1.05))\n",
    "#fig.suptitle(np.unique(complete_df['Imputation']))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('/Users/lirui/Downloads/Cohort_Dementia_Prediction/Survival/Nested_CV_Results/{}/{}/Survival_CV_Boxplots.png'.format(imputation_methods[0], evaluation_strategy), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Get summary statistics in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_report = metrics\n",
    "models_to_report = models\n",
    "feature_sets_to_report = feature_sets\n",
    "imputation_methods_to_report = imputation_methods\n",
    "assert len(imputation_methods)==1\n",
    "\n",
    "imputation_dataset_feature_metric_model_results_df_lst = []\n",
    "for imputation_method in imputation_methods_to_report:\n",
    "    dataset_feature_metric_model_results_df_lst = []\n",
    "    for dataset in [dataset_names[i] for i in list(dataset_names.keys())]:\n",
    "        feature_metric_model_results_df_lst = []\n",
    "        for feature_set in feature_sets_to_report:\n",
    "            metric_model_results = {}\n",
    "            for metric in metrics_to_report:\n",
    "                metric_model_results[metric] = {}\n",
    "                for model in models_to_report:\n",
    "                    selected_df = complete_df.loc[(complete_df['Dataset']==dataset) & (complete_df['Feature Set']==feature_set) & (complete_df['Model']==model) & (complete_df['Imputation']==imputation_method)]\n",
    "                    metric_model_results[metric][model] = {\n",
    "                        'Mean': np.round(np.mean(selected_df[metric]),3),\n",
    "                        'SD': np.round(np.std(selected_df[metric]),3)\n",
    "                    }\n",
    "            feature_metric_model_results_df = pd.DataFrame.from_dict({(i,j): metric_model_results[i][j] \n",
    "                                    for i in metric_model_results.keys() \n",
    "                                    for j in metric_model_results[i].keys()},\n",
    "                                orient='columns')\n",
    "            feature_metric_model_results_df = feature_metric_model_results_df.reset_index(drop=False, col_level=1).rename(columns={'index': 'value'})\n",
    "            feature_metric_model_results_df.insert(0, 'Feature Set', [feature_set]*feature_metric_model_results_df.shape[0])\n",
    "            feature_metric_model_results_df_lst.append(feature_metric_model_results_df)\n",
    "        dataset_feature_metric_model_results_df = pd.concat(feature_metric_model_results_df_lst)\n",
    "        dataset_feature_metric_model_results_df.insert(0, 'Dataset', dataset)\n",
    "        dataset_feature_metric_model_results_df_lst.append(dataset_feature_metric_model_results_df)\n",
    "    imputation_dataset_feature_metric_model_results_df = pd.concat(dataset_feature_metric_model_results_df_lst)\n",
    "    imputation_dataset_feature_metric_model_results_df.insert(0, 'Imputation', imputation_method)\n",
    "    imputation_dataset_feature_metric_model_results_df_lst.append(imputation_dataset_feature_metric_model_results_df)\n",
    "\n",
    "complete_summary_results_df = pd.concat(imputation_dataset_feature_metric_model_results_df_lst)\n",
    "print(complete_summary_results_df.shape)\n",
    "complete_summary_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_summary_results_df.to_csv('/Users/lirui/Downloads/Cohort_Dementia_Prediction/Survival/Nested_CV_Results/{}/{}/Results Summary.csv'.format(imputation_methods_to_report[0], evaluation_strategy), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
